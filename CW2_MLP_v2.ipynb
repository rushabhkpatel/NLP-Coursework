{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework Assignment 2: Measuring Sentence Similarity\n",
    "The purpose of this file is to help you develop your model. You **DON'T** need to submit this file. In the end, you should submit\n",
    "* A report, summarising all your findings and analyses.\n",
    "* For task 1 (MLP-based model), submit two files: **test_mlp.ipynb** which includes the impelementation, and **best_mlp.state_dict**, which is the saved MLP weights.\n",
    "* For task 2 (CNN- or RNN-based model), submit two files: **test_cnn.ipynb** (if you developed a RNN model, change cnn to rnn) which includes the impelementation, and **best_cnn.state_dict**, which is the saved CNN/RNN weights.\n",
    "* For task 3 (additional models), similarly, submit the implementation as well as the saved weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Sent1</th>\n",
       "      <th>Sent2</th>\n",
       "      <th>SimScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>U.S., EU Widen Sanctions On Russia</td>\n",
       "      <td>U.S., EU Boost Sanctions On Russia</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The lawyers advised the judges .</td>\n",
       "      <td>The lawyers advised the judges behind the acto...</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Man kills 4 in Calif. before police shoot him ...</td>\n",
       "      <td>Police: Gunman killed 6 in California shootings</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Someone is playing a piano.</td>\n",
       "      <td>A man is playing a guitar.</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>In an E-mail statement to the Knoxville News S...</td>\n",
       "      <td>I am not giving any consideration to resignati...</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11493</th>\n",
       "      <td>11493</td>\n",
       "      <td>A man is playing piano.</td>\n",
       "      <td>A man is laying on the ground.</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11494</th>\n",
       "      <td>11494</td>\n",
       "      <td>The doctors resigned , or the secretaries supp...</td>\n",
       "      <td>The doctors resigned .</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11495</th>\n",
       "      <td>11495</td>\n",
       "      <td>The artist contacted the banker .</td>\n",
       "      <td>The banker contacted the artist by the student .</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11496</th>\n",
       "      <td>11496</td>\n",
       "      <td>While the professors arrived , the student wai...</td>\n",
       "      <td>The professors arrived .</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11497</th>\n",
       "      <td>11497</td>\n",
       "      <td>The banker avoided the author .</td>\n",
       "      <td>The lawyer and the banker avoided the author .</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11498 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                              Sent1  \\\n",
       "0               0                 U.S., EU Widen Sanctions On Russia   \n",
       "1               1                   The lawyers advised the judges .   \n",
       "2               2  Man kills 4 in Calif. before police shoot him ...   \n",
       "3               3                        Someone is playing a piano.   \n",
       "4               4  In an E-mail statement to the Knoxville News S...   \n",
       "...           ...                                                ...   \n",
       "11493       11493                            A man is playing piano.   \n",
       "11494       11494  The doctors resigned , or the secretaries supp...   \n",
       "11495       11495                  The artist contacted the banker .   \n",
       "11496       11496  While the professors arrived , the student wai...   \n",
       "11497       11497                    The banker avoided the author .   \n",
       "\n",
       "                                                   Sent2  SimScore  \n",
       "0                     U.S., EU Boost Sanctions On Russia      1.00  \n",
       "1      The lawyers advised the judges behind the acto...      0.79  \n",
       "2        Police: Gunman killed 6 in California shootings      0.40  \n",
       "3                             A man is playing a guitar.      0.24  \n",
       "4      I am not giving any consideration to resignati...      0.80  \n",
       "...                                                  ...       ...  \n",
       "11493                     A man is laying on the ground.      0.15  \n",
       "11494                             The doctors resigned .      0.50  \n",
       "11495   The banker contacted the artist by the student .      0.29  \n",
       "11496                           The professors arrived .      0.61  \n",
       "11497     The lawyer and the banker avoided the author .      0.73  \n",
       "\n",
       "[11498 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('cw2_train.csv')\n",
    "dev_data = pd.read_csv('cw2_dev.csv')\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-Trained Embeddings\n",
    "In the sample code below, the Glove pre-trained embedding is used. Feel free to use other embeddings if you find it appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rusha\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "<ipython-input-5-126fad53f96a>:12: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_file, word2vec_glove_file)\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained glove embeddings\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import numpy as np\n",
    "\n",
    "word_vec_dim = 300\n",
    "# specify the loaction of the downloaded glove file\n",
    "link_to_embd = 'https://archive.org/download/glove.6B.50d-300d/glove.6B.300d.txt'\n",
    "\n",
    "path_of_downloaded_files = \"E:/RHUL/CW2-Handout/handout/glove.6B.300d.txt\".format(word_vec_dim)\n",
    "glove_file = datapath(path_of_downloaded_files)\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.300d.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "word_vectors = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
    "\n",
    "oov_vec = np.random.rand(word_vec_dim)\n",
    "\n",
    "def get_sent_word_vecs(word_vectors, sent_words):\n",
    "    vecs = []\n",
    "    for ww in sent_words:\n",
    "        if ww in word_vectors:\n",
    "            vecs.append(word_vectors[ww])\n",
    "        else:\n",
    "            vecs.append(oov_vec)\n",
    "    return np.array(vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Sentence Encoder\n",
    "Below, a simple model to create vector representations for sentences is provided. It first computes the average of the words embeddings, and then passes the average embedding to a fully-connected layer and applies a non-linear activation function to generate the final vector. You should develop more advanced models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the baseline model\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, embd_dim):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.leaky = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fully_connected_layer = nn.Linear(embd_dim, embd_dim)\n",
    "        \n",
    "    def forward(self, sent1_vecs, sent2_vecs):\n",
    "        avg_embd1 = torch.mean(torch.FloatTensor(sent1_vecs), dim=0).unsqueeze(0)\n",
    "        avg_embd2 = torch.mean(torch.FloatTensor(sent2_vecs), dim=0).unsqueeze(0)\n",
    "        \n",
    "        sent1_repr = self.leaky(self.fully_connected_layer(avg_embd1))\n",
    "        sent2_repr = self.leaky(self.fully_connected_layer(avg_embd2))\n",
    "        \n",
    "        sent1_repr_1 = self.leaky(self.fully_connected_layer(avg_embd1))\n",
    "        sent1_repr_1 = self.dropout(sent1_repr_1)\n",
    "        sent2_repr_1 = self.leaky(self.fully_connected_layer(avg_embd2))\n",
    "        sent2_repr_1 = self.dropout(sent2_repr_1)\n",
    "        \n",
    "        sent1_repr_2 = self.leaky(self.fully_connected_layer(avg_embd1))\n",
    "        sent1_repr_2 = self.dropout(sent1_repr_2)\n",
    "        sent2_repr_2 = self.leaky(self.fully_connected_layer(avg_embd2))\n",
    "        sent2_repr_2 = self.dropout(sent2_repr_2)\n",
    "        \n",
    "        sent1_repr_3 = self.leaky(self.fully_connected_layer(avg_embd1))\n",
    "        sent2_repr_3 = self.leaky(self.fully_connected_layer(avg_embd2))\n",
    "        \n",
    "        return sent1_repr_3, sent2_repr_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline\n",
    "The function *train_model* below provides a general pipeline for training the sentence encoder model. You could re-use it for training the model you have developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train_model(train_data, n_epochs, lr, optimizer, loss_fnc, model):\n",
    "    cos_sim = nn.CosineSimilarity()\n",
    "    for epoch_i in tqdm(range(n_epochs)):\n",
    "        ep_loss = []\n",
    "        cnt = 0\n",
    "        for i, entry in tqdm(train_data.sample(frac=1).iterrows()):\n",
    "            cnt += 1\n",
    "            sent1 = entry['Sent1']\n",
    "            sent2 = entry['Sent2']\n",
    "            sent1_embds = get_sent_word_vecs(word_vectors, sent1.split())\n",
    "            sent2_embds = get_sent_word_vecs(word_vectors, sent2.split())\n",
    "\n",
    "            # Step 1: Clear the gradients \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Step 2: Compute the forward pass of the model\n",
    "            sent1_repr, sent2_repr = model(sent1_embds, sent2_embds)\n",
    "            pred_sim = cos_sim(sent1_repr, sent2_repr)\n",
    "            true_sim = torch.FloatTensor([entry['SimScore']])\n",
    "\n",
    "            # Step 3: Compute the loss value that we wish to optimize\n",
    "            loss = loss_fnc(pred_sim, true_sim)\n",
    "            ep_loss.append(loss.detach())\n",
    "\n",
    "            # Step 4: Propagate the loss signal backward\n",
    "            loss.backward()\n",
    "\n",
    "            # Step 5: Trigger the optimizer to perform one update\n",
    "            optimizer.step()\n",
    "\n",
    "            if  cnt%1000 == 0:\n",
    "                print('epoch {}, avg loss until step {}: {}'.format(epoch_i, cnt, np.mean(ep_loss)))\n",
    "\n",
    "        print('\\n======epoch {} loss======'.format(epoch_i),np.mean(ep_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide Hyper-Parameters and Start the Training\n",
    "The hyper-parameters and optimizers provided below are just some examples. You should use appropriate strategy to find the hyper-parameters that you want to use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55813f64044d43058c38fc93a8623505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2bbbcca27844e24bb8bc4e43ffa8260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, avg loss until step 1000: 0.17281030118465424\n",
      "epoch 0, avg loss until step 2000: 0.15705931186676025\n",
      "epoch 0, avg loss until step 3000: 0.15313592553138733\n",
      "epoch 0, avg loss until step 4000: 0.14774677157402039\n",
      "epoch 0, avg loss until step 5000: 0.144443079829216\n",
      "epoch 0, avg loss until step 6000: 0.14011049270629883\n",
      "epoch 0, avg loss until step 7000: 0.1367943286895752\n",
      "epoch 0, avg loss until step 8000: 0.13467946648597717\n",
      "epoch 0, avg loss until step 9000: 0.1317826509475708\n",
      "epoch 0, avg loss until step 10000: 0.13044840097427368\n",
      "epoch 0, avg loss until step 11000: 0.1290140002965927\n",
      "\n",
      "\n",
      "======epoch 0 loss====== 0.12782021\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6558b2ad4d5a4432ba087642648c7d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, avg loss until step 1000: 0.11058720201253891\n",
      "epoch 1, avg loss until step 2000: 0.10245262086391449\n",
      "epoch 1, avg loss until step 3000: 0.10170764476060867\n",
      "epoch 1, avg loss until step 4000: 0.10219619423151016\n",
      "epoch 1, avg loss until step 5000: 0.10232393443584442\n",
      "epoch 1, avg loss until step 6000: 0.10155695676803589\n",
      "epoch 1, avg loss until step 7000: 0.09968837350606918\n",
      "epoch 1, avg loss until step 8000: 0.09895970672369003\n",
      "epoch 1, avg loss until step 9000: 0.09859517216682434\n",
      "epoch 1, avg loss until step 10000: 0.09873271733522415\n",
      "epoch 1, avg loss until step 11000: 0.09800665825605392\n",
      "\n",
      "\n",
      "======epoch 1 loss====== 0.098207995\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa5e523e09a4c6bbf45901c94f0538e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, avg loss until step 1000: 0.08683335781097412\n",
      "epoch 2, avg loss until step 2000: 0.08915097266435623\n",
      "epoch 2, avg loss until step 3000: 0.08825340121984482\n",
      "epoch 2, avg loss until step 4000: 0.08867310732603073\n",
      "epoch 2, avg loss until step 5000: 0.08901476860046387\n",
      "epoch 2, avg loss until step 6000: 0.0885675847530365\n",
      "epoch 2, avg loss until step 7000: 0.08742988109588623\n",
      "epoch 2, avg loss until step 8000: 0.08671735227108002\n",
      "epoch 2, avg loss until step 9000: 0.08707444369792938\n",
      "epoch 2, avg loss until step 10000: 0.08809768408536911\n",
      "epoch 2, avg loss until step 11000: 0.0879080593585968\n",
      "\n",
      "\n",
      "======epoch 2 loss====== 0.08780661\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6daefb0cf534275a483868cc9146281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, avg loss until step 1000: 0.0878792479634285\n",
      "epoch 3, avg loss until step 2000: 0.0882926806807518\n",
      "epoch 3, avg loss until step 3000: 0.08609800040721893\n",
      "epoch 3, avg loss until step 4000: 0.0828108862042427\n",
      "epoch 3, avg loss until step 5000: 0.0830763429403305\n",
      "epoch 3, avg loss until step 6000: 0.08239112049341202\n",
      "epoch 3, avg loss until step 7000: 0.08166351914405823\n",
      "epoch 3, avg loss until step 8000: 0.08155874162912369\n",
      "epoch 3, avg loss until step 9000: 0.08075597137212753\n",
      "epoch 3, avg loss until step 10000: 0.08028236776590347\n",
      "epoch 3, avg loss until step 11000: 0.0798366591334343\n",
      "\n",
      "\n",
      "======epoch 3 loss====== 0.07959144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = BaselineModel(word_vec_dim)\n",
    "loss_fnc = nn.MSELoss()\n",
    "\n",
    "# hyper parameters\n",
    "n_epochs = 4 \n",
    "lr = 1e-2 \n",
    "\n",
    "# init optimizer and scheduler (lr adjustor)\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr) \n",
    "\n",
    "train_model(train_data, n_epochs, lr, optimizer, loss_fnc, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate The Trained Model\n",
    "The function *evaluate_trained_model* defined below tests the performance of a trained model on the dev_set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def evaluate_trained_model(trained_model, dev_data):\n",
    "    pred_scores = []\n",
    "    true_scores = []\n",
    "    cos_sim = nn.CosineSimilarity()\n",
    "    with torch.no_grad(): # let pytorch know that no gradient should be computed\n",
    "        for i, entry in tqdm(dev_data.iterrows()):\n",
    "            sent1 = entry['Sent1']\n",
    "            sent2 = entry['Sent2']\n",
    "            gold_score = entry['SimScore']\n",
    "            sent1_embds = get_sent_word_vecs(word_vectors, sent1.split())\n",
    "            sent2_embds = get_sent_word_vecs(word_vectors, sent2.split())\n",
    "            sent1_repr, sent2_repr = trained_model(sent1_embds, sent2_embds)\n",
    "            pred_sim = cos_sim(sent1_repr, sent2_repr)\n",
    "        \n",
    "            pred_scores.append(pred_sim)\n",
    "            true_scores.append(gold_score)\n",
    "\n",
    "    assert len(true_scores) == len(pred_scores)\n",
    "    squared_errors = [np.square(ts-ps) for (ts, ps) in zip(true_scores, pred_scores)]\n",
    "    print('MSE of the method on the dev set:', np.mean(squared_errors))\n",
    "\n",
    "    # check the distribution (histo gram) of the squared errors\n",
    "    plt.hist(squared_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2328eab5b94a0b9c3e63f8760d1731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MSE of the method on the dev set: 0.08428164\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_trained_model(model, dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save trained model\n",
    "The code below illustrates how to save the trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "info_to_save = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'oov_vec': oov_vec\n",
    "}\n",
    "\n",
    "with open('mlp_model.state_dict', 'wb') as ff:\n",
    "    pickle.dump(info_to_save, ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\RHUL\\\\CW2-Handout\\\\handout'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
